{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project -  Predicting Bike Rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary\n",
    "\n",
    "This Notebook is the conclusion of the **Decision Trees ** course from [dataquest.io](dataquest.io). It is a guided project whose aim is to use all the techniques and skills learnt during the course. Nevertheless we are dealing with real-world data.  \n",
    "We will be working here with data from communal bike stations of Washington DC. The District collects detailed data on the number of bicycles people rent by the hour and day. The Dataset can be downloaded [here](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n",
    "\n",
    "\n",
    "#### Blockquotes usage\n",
    "> I am sometimes using blockquotes as this one, meaning that for the rest of the project I am quoting some elements given by dataquest. For the sake of simplicity and clarity, I estimated that they did not need any reformulation and were immediately usable and convenient for me and the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The file contains 17380 rows, with each row representing the number of bike rentals for a single hour of a single day.\n",
    "\n",
    "Here are the descriptions for the relevant columns:\n",
    "\n",
    "    instant - A unique sequential ID number for each row\n",
    "    dteday - The date of the rentals\n",
    "    season - The season in which the rentals occurred\n",
    "    yr - The year the rentals occurred\n",
    "    mnth - The month the rentals occurred\n",
    "    hr - The hour the rentals occurred\n",
    "    holiday - Whether or not the day was a holiday\n",
    "    weekday - The day of the week (as a number, 0 to 7)\n",
    "    workingday - Whether or not the day was a working day\n",
    "    weathersit - The weather (as a categorical variable)\n",
    "    temp - The temperature, on a 0-1 scale\n",
    "    atemp - The adjusted temperature\n",
    "    hum - The humidity, on a 0-1 scale\n",
    "    windspeed - The wind speed, on a 0-1 scale\n",
    "    casual - The number of casual riders (people who hadn't previously signed up with the bike sharing program)\n",
    "    registered - The number of registered riders (people who had already signed up)\n",
    "    cnt - The total number of bike rentals (casual + registered)\n",
    "\n",
    "#### Objective\n",
    "We'll try to predict the total number of bikes people rented in a given hour. We'll predict the `cnt` column using all of the other columns, except for casual and registered. To accomplish this, we'll create a few different machine learning models and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.3485</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "5        6  2011-01-01       1   0     1   5        0        6           0   \n",
       "6        7  2011-01-01       1   0     1   6        0        6           0   \n",
       "7        8  2011-01-01       1   0     1   7        0        6           0   \n",
       "8        9  2011-01-01       1   0     1   8        0        6           0   \n",
       "9       10  2011-01-01       1   0     1   9        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81     0.0000       3          13   16  \n",
       "1           1  0.22  0.2727  0.80     0.0000       8          32   40  \n",
       "2           1  0.22  0.2727  0.80     0.0000       5          27   32  \n",
       "3           1  0.24  0.2879  0.75     0.0000       3          10   13  \n",
       "4           1  0.24  0.2879  0.75     0.0000       0           1    1  \n",
       "5           2  0.24  0.2576  0.75     0.0896       0           1    1  \n",
       "6           1  0.22  0.2727  0.80     0.0000       2           0    2  \n",
       "7           1  0.20  0.2576  0.86     0.0000       1           2    3  \n",
       "8           1  0.24  0.2879  0.75     0.0000       1           7    8  \n",
       "9           1  0.32  0.3485  0.76     0.0000       8           6   14  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "bike_rentals = pd.read_csv('bike_rental_hour.csv')\n",
    "bike_rentals.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot an histogramm of the `cnt` column to have a look to its distribution.\n",
    "As shown below most of the values are located between 0 and 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAEyCAYAAABdxWyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEnqzAAmSZLU2f8HPvNee9wnfDgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d7e4d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bike_rentals[\"cnt\"].hist(bins=10,grid=False,figsize=(10,5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hum          -0.322911\n",
       "weathersit   -0.142426\n",
       "holiday      -0.030927\n",
       "weekday       0.026900\n",
       "workingday    0.030284\n",
       "windspeed     0.093234\n",
       "mnth          0.120638\n",
       "season        0.178056\n",
       "yr            0.250495\n",
       "instant       0.278379\n",
       "hr            0.394071\n",
       "atemp         0.400929\n",
       "temp          0.404772\n",
       "casual        0.694564\n",
       "registered    0.972151\n",
       "cnt           1.000000\n",
       "Name: cnt, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_rentals.corr()[\"cnt\"].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the correalation between the `cnt` column and the other columns of the data set, we can see that:\n",
    "- obviously registred and casual highly correlate with cnt as, cnt = casual + registred. By the way we won't use these two columns when trying to predict the cnt target later.\n",
    "- temp (the temperature), atemp (the adjusted temperature) and hr have a strong correlation with cnt with coefficients around 0.4\n",
    "- hum (the humidity) is strongly negatively correlated with cnt. (coefficient = -0.32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It can often be helpful to calculate features before applying machine learning models. Features can enhance the accuracy of models by introducing new information, or distilling existing information.\n",
    "\n",
    "> For example, the hr column in bike_rentals contains the hours during which bikes are rented, from 1 to 24. A machine will treat each hour differently, without understanding that certain hours are related. We can introduce some order into the process by creating a new column with labels for morning, afternoon, evening, and night. This will bundle similar times together, enabling the model to make better decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_label(x):\n",
    "    if 6<=x<12:\n",
    "        #morning\n",
    "        return 1\n",
    "    elif 12<=x<18:\n",
    "        #afternoon\n",
    "        return 2\n",
    "    elif 18<=x<24:\n",
    "        #evening\n",
    "        return 3\n",
    "        #night\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_rentals[\"time_label\"]=bike_rentals[\"hr\"].apply(assign_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a metric error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Mean Absolute Error (abreviated for the rest of the project *MEA*) to evaluate further the performances of our different machine learning algorithms. \n",
    "> *MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. Itâ€™s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight. From an interpretation standpoint, MAE is better than RMSE (Root Mean Squared Error), but the latter has the benefit of penalizing large errors more so can be more appropriate in some cases* \n",
    "(see [here](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data into Train and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split our Data set into two sub data sets.\n",
    "- the train data set, containing 80% of the original data set, and which will be used to train our models\n",
    "- the test data set, containing the remaining 20% of the original data set. It will be used to test our previously trained model.\n",
    "\n",
    "To select randomly 80% of the rows of the original data set, we will use the `pandas.Dataframe.sample()` method;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = bike_rentals.sample(frac=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = bike_rentals.loc[~bike_rentals.index.isin(train.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating maching learning models\n",
    "### Applying a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday',\n",
       "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
       "       'casual', 'registered', 'cnt', 'time_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike_rentals.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training and testing our model we have to decide which predictors (also called features) we are going to use.\n",
    "- First we will remove the `instant,dteday,year` columns as they do not bring useful information to train the model\n",
    "- the `casual, registred` columns have to be excluded from the predictors as `cnt` = `casual + registred`\n",
    "- the `atemp` column will be left out, as it is almost perfectly correlated with the `temp` column\n",
    "- the `hour` column will be dropped as we created another feature called `time_label` so that hours are related together\n",
    "- the `mnth` column and the `weekday` : not useful to build a regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = bike_rentals.columns.drop([\"instant\",\"dteday\",\"yr\",\"casual\",\"registered\",\"atemp\",\"hr\",\"weekday\",\"mnth\",\"cnt\"]).tolist()\n",
    "\n",
    "target = \"cnt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.261433554\n"
     ]
    }
   ],
   "source": [
    "lr = linear_model.LinearRegression() \n",
    "fit = lr.fit(train[features],train[\"cnt\"]) \n",
    "test_predictions = lr.predict(test[features])\n",
    "test_mea = mean_absolute_error(test[\"cnt\"],test_predictions)\n",
    "print(test_mea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a MEA = 105 which is quite a large error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.252880223\n"
     ]
    }
   ],
   "source": [
    "features = bike_rentals.columns.drop([\"cnt\",\"instant\",\"dteday\",\"yr\",\"casual\",\"registered\",\"atemp\",\"hr\",\\\n",
    "                                      \"mnth\",\"weekday\",\"holiday\",\"workingday\"]).tolist()\n",
    "lr = linear_model.LinearRegression() \n",
    "fit = lr.fit(train[features],train[\"cnt\"]) \n",
    "test_predictions = lr.predict(test[features])\n",
    "test_mea = mean_absolute_error(test[\"cnt\"],test_predictions)\n",
    "print(test_mea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try now to drop two other predictors : `holiday` and `workingday`  so we are not improving the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Decision Trees\n",
    "\n",
    "We are now going to try another machine learning algorithm : *** Decision Trees***. It will allows us then to choose between the best of the two algorithms depending on the error we get for the decision trees.\n",
    "\n",
    "> Decision trees tend to predict outcomes much more reliably than linear regression models. Because a decision tree is a fairly complex model, it also tends to overfit, particularly when we don't tweak parameters like maximum depth and minimum number of samples per leaf. Decision trees are also prone to instability -- small changes in the input data can result in a very different output model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees (no tweaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = bike_rentals.columns.drop([\"instant\",\"dteday\",\"yr\",\"casual\",\"registered\",\"atemp\",\"hr\",\"weekday\",\"mnth\",\"cnt\"]).tolist()\n",
    "\n",
    "target = \"cnt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.228720752\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree = DecisionTreeRegressor(random_state=1)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Conclusion ***:\n",
    "\n",
    "When training and testing a Regressor Decision Tree without any tweaks we got a MEA_tree = **103**  < MEA_lr = ** 105 **\n",
    "\n",
    "So overall the Decision Tree model is slightly better than the Linear Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees ( tweaking some parameters) \n",
    "\n",
    "Let's try now to adjust some parameters of the `DecisionTree` class to see if it changes the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = bike_rentals.columns.drop([\"instant\",\"dteday\",\"yr\",\"casual\",\"registered\",\"atemp\",\"hr\",\"weekday\",\"mnth\",\"cnt\"]).tolist()\n",
    "\n",
    "target = \"cnt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.969979862\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,min_samples_leaf=1)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.5430559188\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,min_samples_leaf=10)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.3185071616\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,min_samples_leaf=30)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "As we rise the `min_samples_leaf` parameter, ther error is also decreasing. We get a MEA = **84.34**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.3419269528\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,max_depth = 5)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.2715115615\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,max_depth = 10)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.969979862\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,max_depth = 100)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.969979862\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,max_depth = 1000)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : \n",
    "As we rise the value of max_depth, our error is dropping and then increasing again after a certain value (max_depth = 10). We manage to get a similar error than before : MEA = **85.8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.969979862\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,min_samples_split =2)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.3617136555\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,min_samples_split = 5)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.8770010229\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,min_samples_split = 10)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.0037452422\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,min_samples_split = 100)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**:\n",
    "As we rise the value of the  `min_samples_split` parameter, ther error is decreasing, we are improving the model. MEA = **84.41**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.2942270809\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,max_features = 3)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.9053222094\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,max_features = 6)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.969979862\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=1,max_features = 8)\n",
    "tree.fit(train[features], train[\"cnt\"])\n",
    "\n",
    "predictions = tree.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : \n",
    "As we rise the value of max_features, our error is not dropping. We have a MEA = **103**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion  Decision Trees with tweaked parameters\n",
    "\n",
    "By tweaking some paramters for a Regressor Decision Tree we got a MEA_tree_tweaked = **84** < MEA_tree = **103**  < MEA_lr = ** 105 **\n",
    "\n",
    "So overall the adjusted Decision Tree model is better than the basic one and the Linear Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Random Forests\n",
    "\n",
    "We are now going to try improve the Decision Trees algorithm using an ensemble learning method called : **Random Forest** .\n",
    "\n",
    "> Random forests tend to be much more accurate than simple models like linear regression. Due to the way random forests are constructed, they tend to overfit much less than decision trees. Random forests can still be prone to overfitting, though, so it's important to tune parameters like maximum depth and minimum samples per leaf.\n",
    "\n",
    "Let's see right now if the random forest algorithm leads to better performance than random trees alone and the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = bike_rentals.columns.drop([\"instant\",\"dteday\",\"yr\",\"casual\",\"registered\",\"atemp\",\"hr\",\"weekday\",\"mnth\",\"cnt\"]).tolist()\n",
    "\n",
    "target = \"cnt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - no adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.2022313136\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(random_state=1)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the random forest method, with no adjustments at all, we only get a MEA_rf = **84.79** which is quite similar to the lowest MEA we had before with only one single adjusted tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest -  adjusting some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.6336258341\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=1, min_samples_leaf=2)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.5271308974\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=1, min_samples_leaf=5)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.5537228915\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=1, min_samples_leaf=10)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.7356384176\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state=1, min_samples_leaf=50)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : increasing the min_samples_leaf improves our model till a certain value (5) and we manage to get a MEA = **80.6**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.6336258341\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.9239042865\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.7811577373\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=1000, random_state=1, min_samples_leaf=2)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : increasing the n_estimators parameter improves our model, as the error is decreasing, but after 100 estimators, we are not improving anymore the model. Besides the greater n, the longer it takes for the model to train.\n",
    "Still we managed to get a lower MEA than for a decision tree alone. (MEA = **80**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.557702055\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(min_samples_split=10, random_state=1)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.9298542808\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(min_samples_split=30, random_state=1)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.0699949668\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(min_samples_split=50, random_state=1)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** : increasing the min_samples_split parameter improves our model, as the error is decreasing, but after samples = 50, we are not improving anymore the model. Still we managed to get a slightly lower MEA than for a decision tree alone. (MEA = **80.7**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators , min_samples_leaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.7333270769\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100,random_state=1,min_samples_leaf=5)\n",
    "\n",
    "rf.fit(train[features],train[\"cnt\"])\n",
    "predictions = rf.predict(test[features])\n",
    "error = mean_absolute_error(test[\"cnt\"],predictions)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning two parameters at the same time (n_estimators = 100 and min_samples_leaf = 5) we get our lowest MEA = **79.84**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 3 methods, the ***Random Forest*** is the one leading to the best accuracy, it also tends to overfit less than a single decision tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
